---
title: "Keras for R"
author: "Dr. Douglas Ashton"
date: "May 3, 2018"
output: 
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = TRUE, eval = FALSE)
```

## Overview

<div class="row">

<div class="col-md-6">
Dr. Douglas Ashton  
[Mango Solutions](https://www.mango-solutions.com/)  
[\@dougashton](https://twitter.com/dougashton)  
[GitHub repo](https://github.com/mangothecat/keras-workshop)  
</div>

<div class="col-md-6">
![](images/ashton.jpg)
</div>

</div>

## Prerequisites

### Installing Keras and Package Dependencies
```{r install-deps}
install.packages('caret')
install.packages('tidyverse', dependencies=TRUE)
install.packages('keras')

library(caret)
library(tidyverse)

library(keras)
install_keras() # install keras and tensorflow dependencies
is_keras_available()
```

### Download Data
```{r download-data}
root <- rprojroot::find_rstudio_root_file()
dataDir <- file.path(root, 'data')

# xIris.rds
download.file(
    'https://github.com/MangoTheCat/keras-workshop/tree/master/data/xIris.rds',
    destfile=file.path(dataDir, 'xIris.rds'),
    mode='wb')

# yIris.rds
download.file(
    'https://github.com/MangoTheCat/keras-workshop/tree/master/data/yIris.rds',
    destfile=file.path(dataDir, 'yIris.rds'),
    mode='wb')

# xWalk.rds
download.file(
    'https://github.com/MangoTheCat/keras-workshop/tree/master/data/xWalk.rds',
    destfile=file.path(dataDir, 'xWalk.rds'),
    mode='wb')

# yWalk.rds
download.file(
    'https://github.com/MangoTheCat/keras-workshop/tree/master/data/yWalk.rds',
    destfile=file.path(dataDir, 'yWalk.rds'),
    mode='wb')
```

## What is Deep Learning

- Feataures (inputs) - try to go wide
- Transformed features - turn inputs into something more representative and useful to predictive modeling
- Algo to munch the features
- Target (output)
- Need to learn how to 
    - Feature transform
    - Build the algo
- DL good for
    - Unstructured data, where features are learned rather than designed
        - Head of kaggle said ML better/winning on structured data
    - Big data
        - Needs lots of data
    - Familiar
        - Trained networks can be used on new problems

### Neural Networks

- Nodes (neurons) & edges
- Nodes are built up in horizontal layers, contain values
- Edges are weighted, feeding values from one layer to next, usually aggregating in next layer
- Input layer -> thru hidden layers (increasingly abstracted) -> to the output layer

## Iris Neural Network

- Four features
    - Sepal length
    - Sepal width
    - Petal width
    - Petal length
- One target: species (setosa, versicolo, etc.)

### Data Preparation

Building a neural network model requires _training_ and _test_ data sets.
Often, and preferable, the training data is significantly larger than the test data,
as NNs benefit from larger training sets making "better" (data quality aside) models.

In this first model, against the `iris` dataset, we'll use 80% for training, with the
remaining 20% as test data. The prepared dataset is available in the above-downloaded
`xIris.rds` and `yIris.rds` files and can beloaded directly via

```{r prepared-data}
xIris <- readRDS("data/xIris.rds")
yIris <- readRDS("data/yIris.rds")
```

The below steps show the preparations done on that data.

First, use the `caret` package to partition the data, by ID, into training and test sets.
```{r partition-data}
trainID <- createDataPartition(iris$Species, p = 0.8) 
```

Use the `trainID` partition list to slice `iris` data into training and test datasets.
Thesee two sets are then combined into a `fullData` list.  
```{r }
trainingData <- iris %>%
  slice(trainID$Resample1)

testData <- iris %>%
  slice(-trainID$Resample1)

fullData <- list(train = trainingData, 
                 test = testData)
```

The `iris` dataset is comprised of several numerical variables that, for each data point,
are related to on or another categorical `Species` value. To build a model that predicts
`Species` based on the _sepal_ and _petal_ _lengths_ and _widths_, a set of boolen
dummy variables have to be made for each of the categorical values in `Species`.
This process uses the `caret::dummyVars()` and `purrr::map()` functions.  
```{r dummy-vars}
dummy <- dummyVars(~ Species, data = iris)
irisDummy <- map(fullData, predict, object = dummy)
head(irisDummy$train)
```

Further cleanup of the dataset 
1. removes the non-numeric `Species` response variable,
1. scales and centers the `test` and `training` data, and
1. replaces all "NA" values with "0".

```{r data-cleanup}
numericIris <- map(fullData, select_if, is.numeric)
scaledIris <- map(numericIris, scale)
map(scaledIris, replace_na, replace = 0)
head(scaledIris$train)
```

We complete data processing by separating our combined `train` and `test` data
into separate predictor (`xIris`) and response (`yIris`) matrices.  
```{r create-matrices}
xIris <- map(scaledIris, as.matrix)
yIris <- map(irisDummy, as.matrix)
```

### Build and Train the Model

Build the model.  
```{r build-model}
model <- keras_model_sequential()
```

Add layers to the neural network.  
```{r add-layers}
model %>%
  layer_dense(units = 10, input_shape = 4) %>%
  layer_dense(units = 3, activation = 'softmax')
```

Compile the model, which 
```{r compile-model}
model %>% compile(
  optimizer = 'rmsprop',
  loss = 'categorical_crossentropy',
  metrics = 'accuracy'
)
```

Train the model
```{r train-model}
history <- model %>% fit(xIris$train, 
                     yIris$train, 
                     epochs = 100, 
                     validation_data = list(xIris$test, yIris$test))
summary(model)
plot(history)
```


### Evaluate the Model

```{r}
model %>% 
  evaluate(xIris$test, yIris$test)


model %>% 
  predict(xIris$test)

model %>%
  predict_classes(xIris$test) 
```


Controlling Layers
```{r}
model <- keras_model_sequential()

model %>%
  layer_dense(units = 10, input_shape = 4) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 3, activation = 'softmax')

model %>% compile(
  optimizer = 'rmsprop',
  loss = 'categorical_crossentropy',
  metrics = 'accuracy'
)

model %>% fit(xIris$train, 
              yIris$train, 
              epochs = 100, 
              validation_data = list(xIris$test, yIris$test))
```

## TensorFlow, Keras, and R
__Reference__: _Deep Learning with R_, by Francois Challet

#### TensorFlow
- Turns models/equations into numerical solvers doing the heavy linear algebra, via transformation into a graph structure
- Not only for neural networks, general numerical solver

#### R and TensorFlow
- [TensorFlow docs from RStudio](http://tensorflow.rstudio.com)
- Use of the `reticulate` package to wrap the required Python packages
- Not much overhead to bring up Python when installing TF

#### Keras
- [Keras docs from RStudio](http://keras.rstudio.com)
- Higher level on top of TF, specifically for neural networks

## How it all fits together
- We will write against Keras in R
- Keras will turn that into neural network math
- TensorFlow receives math from Keras to compute

### Preparation

To execute this model locally, we'll limit computational power by providing a
seeded session for TensorFlow, where the following call disables both GPU
execution and parallelism on the CPU.  
```{r seed-tf-session}
use_session_with_seed(1234)
```

Next, load the test data downloaded above, which has already been prepped with
`train` and `test` subsets of the data.  
```{r load-walk-data}
xWalk <- readRDS("data/xWalk.rds")
yWalk <- readRDS("data/yWalk.rds")
```

### Build a Convolutional Neural Network (CNN) Model

Make an empty sequential model, where layers feed directly into subsequent layers.  
```{r start-empty-seq-model}
model <- keras_model_sequential()
```

We'll next build our CNN, by adding layer definitions onto the model scaffolding.
1. 
```{r build-cnn}
model %>%
	  layer_conv_1d(filters = 40, kernel_size = 30, strides = 2,
			                activation = "relu", input_shape = c(260, 3)) %>%
  layer_max_pooling_1d(pool_size = 2) %>%
    layer_conv_1d(filters = 40, kernel_size = 10, activation = "relu") %>%
      layer_max_pooling_1d(pool_size = 2) %>%
        layer_flatten() %>%
	  layer_dense(units = 100, activation = "sigmoid") %>%
	    layer_dense(units = 15, activation = "softmax")
model
```

Compile
```{r compile-cnn}
model %>%
	  compile(loss = "categorical_crossentropy",
		    optimizer = "adam",
				metrics = c("accuracy"))
```

Run
```{r run-cnn}
history <- model %>% fit(xWalk$train, yWalk$train,
		epochs = 15, 
		batch_size = 128, 
		validation_split = 0.3,
		verbose = 1)
```

Evaluate
```{r eval-cnn}
model %>% 
	  evaluate(xWalk$test, yWalk$test, verbose = 0)
```
